{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":1,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"##########################Load Libraries  ####################################\nimport pandas as pd\nimport numpy as np\nimport dask.dataframe as dd\npd.set_option('display.max_columns', 500)\npd.set_option('display.max_rows', 500)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport lightgbm as lgb\nfrom sklearn import preprocessing, metrics\nfrom ipywidgets import widgets, interactive\nimport gc\nimport joblib\nimport warnings\nwarnings.filterwarnings('ignore')\nfrom datetime import datetime, timedelta \nfrom typing import Union\nfrom tqdm.notebook import tqdm_notebook as tqdm\nfrom itertools import cycle\nimport datetime as dt\nfrom torch.autograd import Variable\nimport random \nimport os\nfrom matplotlib.pyplot import figure\nfrom fastprogress import master_bar, progress_bar\nimport torch\nimport torch.nn as nn\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom sklearn.preprocessing import MinMaxScaler\nimport time \nfrom torch.utils.data import Dataset\nfrom sklearn.metrics import mean_squared_error\nimport torch \nfrom numpy import array\n%matplotlib inline\n\n#from gensim.models import Word2Vec\n#import gensim.downloader as api\n\npd.set_option('max_columns', 50)\nplt.style.use('bmh')\ncolor_pal = plt.rcParams['axes.prop_cycle'].by_key()['color']\ncolor_cycle = cycle(plt.rcParams['axes.prop_cycle'].by_key()['color'])","execution_count":2,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Univariate LSTM Models\nLSTMs can be used to model univariate time series forecasting problems.\n\nThese are problems comprised of a single series of observations and a model is required to learn from the series of past observations to predict the next value in the sequence.\n\nWe will demonstrate a number of variations of the LSTM model for univariate time series forecasting.\n\nThis section is divided into six parts; they are:\n\n    1. Data Preparation\n    2. Vanilla LSTM\n    3. Stacked LSTM\n    4. Bidirectional LSTM\n    5. CNN LSTM\n    6. ConvLSTM\nEach of these models are demonstrated for one-step univariate time series forecasting, but can easily be adapted and used as the input part of a model for other types of time series forecasting problems."},{"metadata":{},"cell_type":"markdown","source":"## 1. Data Preparation"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Consider a given univariate sequence:\n[10, 20, 30, 40, 50, 60, 70, 80, 90]","execution_count":3,"outputs":[{"output_type":"execute_result","execution_count":3,"data":{"text/plain":"[10, 20, 30, 40, 50, 60, 70, 80, 90]"},"metadata":{}}]},{"metadata":{},"cell_type":"markdown","source":"We can divide the sequence into multiple input/output patterns called samples, where three time steps are used as input and one time step is used as output for the one-step prediction that is being learned."},{"metadata":{"trusted":true},"cell_type":"code","source":"# The split_sequence() function below implements this behavior \n# and will split a given univariate sequence into multiple samples where each sample has a specified number of time steps and the output is a single time step.\ndef split_sequence(sequence, n_steps):\n    X, y = list(), list()\n    for i in range(len(sequence)):\n        # find the end of this pattern\n        end_ix = i + n_steps\n        # check if we are beyond the sequence\n        if end_ix > len(sequence)-1:\n            break\n        # gather input and output parts of the pattern\n        seq_x, seq_y = sequence[i:end_ix], sequence[end_ix]\n        X.append(seq_x)\n        y.append(seq_y)\n    return array(X), array(y)","execution_count":4,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# define input sequence\nraw_seq = [10, 20, 30, 40, 50, 60, 70, 80, 90]\n# choose a number of time steps\nn_steps = 3\n# split into samples\nX, y = split_sequence(raw_seq, n_steps)\n# summarize the data\nfor i in range(len(X)):\n    print(X[i], y[i])","execution_count":5,"outputs":[{"output_type":"stream","text":"[10 20 30] 40\n[20 30 40] 50\n[30 40 50] 60\n[40 50 60] 70\n[50 60 70] 80\n[60 70 80] 90\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"# univariate lstm example\nfrom numpy import array\nfrom keras.models import Sequential\nfrom keras.layers import LSTM\nfrom keras.layers import Dense","execution_count":6,"outputs":[{"output_type":"stream","text":"Using TensorFlow backend.\n","name":"stderr"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"# define model\nn_features = 1\nmodel = Sequential()\nmodel.add(LSTM(50,activation='relu',input_shape=(n_steps,n_features)))\nmodel.add(Dense(1))\nmodel.compile(optimizer='adam',loss='mse')","execution_count":7,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# reshape from[sample,timesteps] into [samples,timesteps,features]\nn_features = 1\nX = X.reshape((X.shape[0],X.shape[1],n_features))","execution_count":8,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# fit model\nmodel.fit(X,y,epochs =200, verbose=0)","execution_count":9,"outputs":[{"output_type":"execute_result","execution_count":9,"data":{"text/plain":"<keras.callbacks.callbacks.History at 0x7f534440c510>"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"# demostration prediction\nx_input = array([70,80,90])\nx_input = x_input.reshape(1,n_steps,n_features)\nyhat = model.predict(x_input,verbose=0)","execution_count":10,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(yhat)","execution_count":11,"outputs":[{"output_type":"stream","text":"[[103.00992]]\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"## 2.Stacked LSTM"},{"metadata":{"trusted":true},"cell_type":"code","source":"# define model\nmodel = Sequential()\nmodel.add(LSTM(50, activation='relu', return_sequences=True, input_shape=(n_steps, n_features)))\nmodel.add(LSTM(50, activation='relu'))\nmodel.add(Dense(1))\nmodel.compile(optimizer='adam', loss='mse')","execution_count":12,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# fit model\nmodel.fit(X,y,epochs =200, verbose=0)","execution_count":13,"outputs":[{"output_type":"execute_result","execution_count":13,"data":{"text/plain":"<keras.callbacks.callbacks.History at 0x7f533c36ead0>"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"# demostration prediction\nx_input = array([70,80,90])\nx_input = x_input.reshape(1,n_steps,n_features)\nyhat = model.predict(x_input,verbose=0)","execution_count":14,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(yhat)","execution_count":15,"outputs":[{"output_type":"stream","text":"[[103.606125]]\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"## 3.Bidirectional LSTM"},{"metadata":{"trusted":true},"cell_type":"code","source":"from keras.layers import Bidirectional","execution_count":16,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# define model\nmodel = Sequential()\nmodel.add(Bidirectional(LSTM(50, activation='relu'), input_shape=(n_steps, n_features)))\nmodel.add(Dense(1))\nmodel.compile(optimizer='adam', loss='mse')","execution_count":17,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# fit model\nmodel.fit(X, y, epochs=200, verbose=0)\n# demonstrate prediction\nx_input = array([70, 80, 90])\nx_input = x_input.reshape((1, n_steps, n_features))\nyhat = model.predict(x_input, verbose=0)\n","execution_count":18,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(yhat)","execution_count":19,"outputs":[{"output_type":"stream","text":"[[101.80447]]\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"## 4.CNN LSTM"},{"metadata":{"trusted":true},"cell_type":"code","source":"from keras.layers import Flatten\nfrom keras.layers import TimeDistributed\nfrom keras.layers.convolutional import Conv1D\nfrom keras.layers.convolutional import MaxPooling1D","execution_count":20,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# define input sequence\nraw_seq = [10, 20, 30, 40, 50, 60, 70, 80, 90]\n# choose a number of time steps\nn_steps = 4\n# split into samples\nX, y = split_sequence(raw_seq, n_steps)\n# reshape from [samples, timesteps] into [samples, subsequences, timesteps, features]\nn_features = 1\nn_seq = 2\nn_steps = 2\nX = X.reshape((X.shape[0], n_seq, n_steps, n_features))\n# define model\nmodel = Sequential()\nmodel.add(TimeDistributed(Conv1D(filters=64, kernel_size=1, activation='relu'), input_shape=(None, n_steps, n_features)))\nmodel.add(TimeDistributed(MaxPooling1D(pool_size=2)))\nmodel.add(TimeDistributed(Flatten()))\nmodel.add(LSTM(50, activation='relu'))\nmodel.add(Dense(1))\nmodel.compile(optimizer='adam', loss='mse')\n# fit model\nmodel.fit(X, y, epochs=500, verbose=0)\n# demonstrate prediction\nx_input = array([60, 70, 80, 90])\nx_input = x_input.reshape((1, n_seq, n_steps, n_features))\nyhat = model.predict(x_input, verbose=0)\n","execution_count":21,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(yhat)","execution_count":22,"outputs":[{"output_type":"stream","text":"[[100.56886]]\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"## 5.ConvLSTM"},{"metadata":{"trusted":true},"cell_type":"code","source":"from keras.layers import Flatten\nfrom keras.layers import ConvLSTM2D","execution_count":23,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# define input sequence\nraw_seq = [10, 20, 30, 40, 50, 60, 70, 80, 90]\n# choose a number of time steps\nn_steps = 4\n# split into samples\nX, y = split_sequence(raw_seq, n_steps)\n# reshape from [samples, timesteps] into [samples, timesteps, rows, columns, features]\nn_features = 1\nn_seq = 2\nn_steps = 2\nX = X.reshape((X.shape[0], n_seq, 1, n_steps, n_features))\n# define model\nmodel = Sequential()\nmodel.add(ConvLSTM2D(filters=64, kernel_size=(1,2), activation='relu', input_shape=(n_seq, 1, n_steps, n_features)))\nmodel.add(Flatten())\nmodel.add(Dense(1))\nmodel.compile(optimizer='adam', loss='mse')\n# fit model\nmodel.fit(X, y, epochs=500, verbose=0)\n# demonstrate prediction\nx_input = array([60, 70, 80, 90])\nx_input = x_input.reshape((1, n_seq, 1, n_steps, n_features))\nyhat = model.predict(x_input, verbose=0)","execution_count":24,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(yhat)","execution_count":25,"outputs":[{"output_type":"stream","text":"[[103.93673]]\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"#neural network\nfrom numpy import exp, array, random, dot","execution_count":48,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class NeuralNetwork():\n    def __init__(self):\n        # Seed the random nimber generator, so it generates the same numbers\n        # every time the program runs.\n        random.seed(1)\n        \n        # We model a single neuron, with 3 output connections and 1 output connection\n        # We assign random weights to a 3x1 matrix, with values in the range -1 to 1\n        # and mean 0.\n        self.synaptic_weights = 2*random.random((3,1))-1\n        \n    # define sigmoid - activate function\n    def sigmoid(self,x):\n        return 1/(1+np.exp(-x))\n    \n    def sigmoid_derivative(self,x):\n        return np.exp(-x)/(1+np.exp(-x))**2\n        \n    # We train the neural network through a process o trial and error.\n    # Adjusting the synaptic weights each time.\n    def train(self,training_set_input,training_set_output,num_of_traning_iterations):\n        for i in range(num_of_traning_iterations):\n            output = self.activate_func(training_set_input)\n            \n            # calculate the error (The different between the desied output\n            # and the predicted output)\n            \n            error = training_set_output-output\n            \n            # Multiply the error by the input and again by the gradient of the Sigmoid curve\n            # This means less confident weight are adjusted more.\n            # This means input, which are zero, do not cause changes to the weight.\n            adjustment = dot(training_set_input.T,error*self.sigmoid_derivative(output))\n            \n            # Adjust the weight\n            self.synaptic_weights += adjustment\n    def activate_func(self,inputs):\n        return self.sigmoid(dot(inputs,self.synaptic_weights))\n","execution_count":49,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":" # Intialise a signle neural network.\nneural_network = NeuralNetwork()\n    \nprint(\"Random starting synaptic weights:\")\nprint(neural_network.synaptic_weights)\n    \n# The training set: 4 examples, each consisting of 3 input values\n# and 1 output values.\n    \ntraining_set_input = array([[0,0,1],\n                               [1,1,1],\n                               [1,0,1],\n                               [0,1,1]])\ntraining_set_output = array([[0,1,1,0]]).T\n    \n# Train the neural network  using a training set.\n# Do it 10,000 times and make samll adjustments each times.\nneural_network.train(training_set_input,training_set_output,100)\n    \nprint(\"Mew synaptic weights after training:\")\nprint(neural_network.synaptic_weights)\n    \n# Test the neural network with a new situation.\nprint(\"considering new situation[1,0,0]->?:\")\nprint(neural_network.activate_func(array([1,0,0])))","execution_count":51,"outputs":[{"output_type":"stream","text":"Random starting synaptic weights:\n[[-0.16595599]\n [ 0.44064899]\n [-0.99977125]]\nMew synaptic weights after training:\n[[19.64772187]\n [-0.40556114]\n [-9.53843554]]\nconsidering new situation[1,0,0]->?:\n[1.]\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"a = [[1,2,3],\n    [4,5,6],\n    [7,8,9]]\nb = [[1,1,1],\n    [1,1,1],\n    [1,1,1]]","execution_count":34,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"np.dot(a,b)","execution_count":35,"outputs":[{"output_type":"execute_result","execution_count":35,"data":{"text/plain":"array([[ 6,  6,  6],\n       [15, 15, 15],\n       [24, 24, 24]])"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}