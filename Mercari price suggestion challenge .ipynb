{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load in \n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "import gc\n",
    "import glob\n",
    "import os\n",
    "import json\n",
    "\n",
    "import pprint\n",
    "\n",
    "from joblib import Parallel, delayed\n",
    "from tqdm import tqdm\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model, load_model\n",
    "from tensorflow.keras.applications.densenet import preprocess_input, DenseNet121\n",
    "from tensorflow.keras.layers import GlobalAveragePooling2D, Input, Lambda, AveragePooling1D, \\\n",
    "    MaxPooling1D, Dense, BatchNormalization, Dropout, Embedding, Reshape, Concatenate\n",
    "from tensorflow.keras import losses\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras import regularizers\n",
    "import tensorflow.keras.backend as K\n",
    "\n",
    "from sklearn.model_selection import GroupKFold\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD, NMF\n",
    "from sklearn.metrics import cohen_kappa_score, mean_squared_error\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "import scipy as sp\n",
    "\n",
    "from collections import Counter\n",
    "from functools import partial\n",
    "from math import sqrt\n",
    "\n",
    "from sklearn.metrics import cohen_kappa_score, mean_squared_error\n",
    "from sklearn.metrics import confusion_matrix as sk_cmatrix\n",
    "import lightgbm as lgb\n",
    "\n",
    "from gensim.models import KeyedVectors\n",
    "\n",
    "import nltk\n",
    "import string\n",
    "import re\n",
    "import pickle\n",
    "#import lda\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set(style=\"white\")\n",
    "\n",
    "from nltk.stem.porter import *\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction import stop_words\n",
    "\n",
    "from collections import Counter\n",
    "from wordcloud import WordCloud\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "\n",
    "import plotly.offline as py\n",
    "py.init_notebook_mode(connected=True)\n",
    "import plotly.graph_objs as go\n",
    "import plotly.tools as tls\n",
    "%matplotlib inline\n",
    "\n",
    "import bokeh.plotting as bp\n",
    "from bokeh.models import HoverTool, BoxSelectTool\n",
    "from bokeh.models import ColumnDataSource\n",
    "from bokeh.plotting import figure, show, output_notebook\n",
    "#from bokeh.transform import factor_cmap\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder, MinMaxScaler, StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "import math\n",
    "from subprocess import check_output\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import logging\n",
    "logging.getLogger(\"lda\").setLevel(logging.WARNING)\n",
    "\n",
    "import zipfile\n",
    "from subprocess import check_output\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "# Input data files are available in the \"../input/\" directory.\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# Any results you write to the current directory are saved as output."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Exploratory Data Analysis**\n",
    "On the first look at the data, besides the unique identifier (item_id), there are 7 variables in this model. This notebook will sequentially go through each of them with a brief statistical summary. \n",
    "\n",
    "1. **Numerical/Continuous Features**\n",
    "    1. price: the item's final bidding price. This will be our reponse / independent variable that we need to predict in the test set\n",
    "    2. shipping cost     \n",
    " \n",
    "1. **Categorical Features**: \n",
    "    1. shipping cost: A binary indicator, 1 if shipping fee is paid by seller and 0 if it's paid by buyer\n",
    "    2. item_condition_id: The condition of the items provided by the seller\n",
    "    1. name: The item's name\n",
    "    2. brand_name: The item's producer brand name\n",
    "    2. category_name: The item's single or multiple categories that are separated by \"\\\" \n",
    "    3. item_description: A short description on the item that may include removed words, flagged by [rm]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('/kaggle/input/mercari/train.tsv', sep='\\t')\n",
    "test = pd.read_csv('/kaggle/input/mercari/test.tsv', sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train.shape)\n",
    "print(test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Missing values of data train\\n',train.isnull().sum())\n",
    "print('----------------------')\n",
    "print('Missing values of data test\\n',test.isnull().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_mis_values(dataset):\n",
    "    dataset.category_name.fillna(value = 'missing', inplace = True)\n",
    "    dataset.brand_name .fillna(value = 'missing', inplace = True)\n",
    "    dataset.item_description .fillna(value = 'missing', inplace = True)\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = process_mis_values(train)\n",
    "test = process_mis_values(test)\n",
    "print(train.isnull().sum())\n",
    "print(test.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check types of variable\n",
    "# Numeric variable\n",
    "number = [f for f in train.columns if train.dtypes[f] != 'object']\n",
    "# Object variable\n",
    "objects = [f for f in train.columns if train.dtypes[f] == 'object']\n",
    "print(number)\n",
    "print(objects)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Process categorical variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "le = LabelEncoder()\n",
    "le.fit(np.hstack([train.category_name,test.category_name]))\n",
    "train.category_name = le.transform(train.category_name)\n",
    "test.category_name = le.transform(test.category_name)\n",
    "\n",
    "le.fit(np.hstack([train.brand_name,test.brand_name]))\n",
    "train.brand_name = le.transform(train.brand_name)\n",
    "test.brand_name = le.transform(test.brand_name)\n",
    "\n",
    "del le"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Process text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Text to sequence processing\n",
    "token = Tokenizer()\n",
    "raw_text = np.hstack([train.item_description.str.lower(), test.item_description.str.lower()])\n",
    "token.fit_on_texts(raw_text)\n",
    "\n",
    "train['seq_item_descri'] = token.texts_to_sequences(train.item_description.str.lower())\n",
    "test['seq_item_descri'] = token.texts_to_sequences(test.item_description.str.lower())\n",
    "train['seq_name'] = token.texts_to_sequences(train.name.str.lower())\n",
    "test['seq_name'] = token.texts_to_sequences(test.name.str.lower())\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(len(train.seq_item_descri.max()))\n",
    "#print(type(train.seq_name[2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(token.word_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sequence Analyst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_name_seq = np.max([np.max(train.seq_name.apply(lambda x: len(x))),np.max(test.seq_name.apply(lambda x:len(x)))])\n",
    "max_seq_item_descri = np.max([np.max(train.seq_item_descri.apply(lambda x: len(x))),np.max(test.seq_item_descri.apply(lambda x:len(x)))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('max name seq', max_name_seq)\n",
    "print('max seq item descri',max_seq_item_descri )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.seq_name.apply(lambda x: len(x)).hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.seq_item_descri.apply(lambda x: len(x)).hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Base on the histograms, we select the next lengths\n",
    "max_name_seq = 10\n",
    "max_descri = 100\n",
    "max_text = np.max([np.max(train.seq_name.max()),\n",
    "                np.max(test.seq_name.max()),\n",
    "                np.max(train.seq_item_descri.max()),\n",
    "                np.max(test.seq_item_descri.max())])+2\n",
    "max_categoty = np.max([np.max(train.category_name),np.max(test.category_name)])+1\n",
    "max_brand = np.max([np.max(train.brand_name), np.max(train.brand_name)])+1\n",
    "max_condition = np.max([train.item_condition_id.max(), test.item_condition_id.max()])+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(max_text)\n",
    "print(max_categoty)\n",
    "print(max_brand)\n",
    "print(max_condition)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#SCALE target variable\n",
    "train[\"target\"] = np.log(train.price+1)\n",
    "target_scaler = MinMaxScaler(feature_range=(-1, 1))\n",
    "train[\"target\"] = target_scaler.fit_transform(train.target.values.reshape(-1,1))\n",
    "pd.DataFrame(train.target).hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#EXTRACT DEVELOPTMENT TEST\n",
    "#dtrain, dvalid = train_test_split(train[['train_id','brand_name','category_name','item_condition_id','price','shipping','seq_item_descri','seq_name']], random_state=123, train_size=0.99)\n",
    "#print(dtrain.shape)\n",
    "#print(dvalid.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_keras_data(dataset):\n",
    "    df_name = pd.DataFrame(data=pad_sequences(dataset.seq_name, maxlen=max_name_seq),index = train['train_id'], columns=['name_factor' + '_' + str(k) for k in range(max_name_seq)])\n",
    "    df_name = df_name.reset_index()\n",
    "    df_item = pd.DataFrame(data=pad_sequences(dataset.seq_item_descri, maxlen=max_descri),index = train['train_id'], columns=['item_factor' + '_' + str(k) for k in range(max_descri)])\n",
    "    df_item = df_item.reset_index()\n",
    "    X = dataset[['train_id','item_condition_id','brand_name','category_name','shipping','target']]\n",
    "    X_1 = pd.merge(df_name,df_item, on = 'train_id')\n",
    "    X_final = pd.merge(X,X_1, on = 'train_id')\n",
    "    X_final = X_final.drop('train_id',axis = 1)\n",
    "    return X_final\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = get_keras_data(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_keras_data_stg2(dataset):\n",
    "    df_name = pd.DataFrame(data=pad_sequences(dataset.seq_name, maxlen= max_name_seq),index = test['test_id'], columns=['name_factor' + '_' + str(k) for k in range(max_name_seq)])\n",
    "    df_name = df_name.reset_index()\n",
    "    df_item = pd.DataFrame(data=pad_sequences(dataset.seq_item_descri, maxlen= max_descri),index = test['test_id'], columns=['item_factor' + '_' + str(k) for k in range(max_descri)])\n",
    "    df_item = df_item.reset_index()\n",
    "    X = dataset[['test_id','item_condition_id','brand_name','category_name','shipping']]\n",
    "    X_1 = pd.merge(df_name,df_item, on = 'test_id')\n",
    "    X_final = pd.merge(X,X_1, on = 'test_id')\n",
    "    X_final = X_final.drop('test_id',axis = 1)\n",
    "    return X_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_stg2 = get_keras_data_stg2(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_stg2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model base\n",
    "# LightGBM\n",
    "import lightgbm as lgb\n",
    "\n",
    "params = {'application': 'regression',\n",
    "          'boosting': 'gbdt',\n",
    "          'metric': 'rmse',\n",
    "          'num_leaves': 70,\n",
    "          'max_depth': 9,\n",
    "          'learning_rate': 0.5,\n",
    "          'bagging_fraction': 0.85,\n",
    "          'feature_fraction': 0.8,\n",
    "          'min_split_gain': 0.02,\n",
    "          'min_child_samples': 150,\n",
    "          'min_child_weight': 0.02,\n",
    "          'lambda_l2': 0.0475,\n",
    "          'verbosity': -1,\n",
    "          'data_random_seed': 17,\n",
    "          'tree learner':'feature'\n",
    "          }\n",
    "\n",
    "# Additional parameters:\n",
    "early_stop = 1000\n",
    "verbose_eval = 50\n",
    "num_rounds = 5000\n",
    "n_splits = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def LGBM_train(X_train, X_test):\n",
    "    \n",
    "    kfold = KFold(n_splits, random_state = 1337 )\n",
    "    oof_train = np.zeros([X_train.shape[0]])\n",
    "    oof_test = np.zeros([X_test.shape[0], n_splits])\n",
    "    i = 0\n",
    "    for train_index, valid_index in kfold.split(X_train, X_train['target'].values):\n",
    "    \n",
    "        X_tr = X_train.iloc[train_index, :]\n",
    "        X_val = X_train.iloc[valid_index, :]\n",
    "\n",
    "        y_tr = X_tr['target'].values\n",
    "        X_tr = X_tr.drop(['target'], axis=1)\n",
    "\n",
    "        y_val = X_val['target'].values\n",
    "        X_val = X_val.drop(['target'], axis=1)\n",
    "    \n",
    "#         print('\\ny_tr distribution: {}'.format(Counter(y_tr)))\n",
    "\n",
    "        d_train = lgb.Dataset(X_tr, label=y_tr)\n",
    "        d_valid = lgb.Dataset(X_val, label=y_val)\n",
    "        watchlist = [d_train, d_valid]\n",
    "\n",
    "        print('training LGB:')\n",
    "        model = lgb.train(params,\n",
    "                          train_set=d_train,\n",
    "                          num_boost_round=num_rounds,\n",
    "                          valid_sets=watchlist,\n",
    "                          verbose_eval=verbose_eval,\n",
    "                          early_stopping_rounds=early_stop)\n",
    "\n",
    "        val_pred = model.predict(X_val, num_iteration = model.best_iteration)\n",
    "        test_pred = model.predict(X_test, num_iteration = model.best_iteration)\n",
    "\n",
    "        oof_train[valid_index] = val_pred\n",
    "        oof_test[:,i] = test_pred\n",
    "        i +=1\n",
    "        \n",
    "    return oof_train, oof_test, model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "oof_train, oof_test, model = LGBM_train(X,X_stg2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_preds = target_scaler.inverse_transform(oof_test)\n",
    "val_preds = np.expm1(val_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = pd.DataFrame(data = val_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A['final_price'] = A.mean(axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission = pd.DataFrame({'test_id': test['test_id'], 'price': A['final_price']})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission.to_csv(\"submission.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_log_error\n",
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "oof_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_rmsle(y_true, y_pred):\n",
    "    return np.sqrt(mean_squared_log_error(np.expm1(y_true), np.expm1(y_pred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_rmsle(train['target'], oof_train)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
